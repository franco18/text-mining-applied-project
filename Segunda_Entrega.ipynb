{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob # libreria para extraer la ruta de los archivos\n",
    "import re # libreria para manejos de expresiones regulares\n",
    "import collections #### para poder contar los hash\n",
    "import pandas as pd # libreria para manejo de bases datos\n",
    "import numpy as np # libreria para manejo de vectores y arreglos\n",
    "import json #libreria para leer los metadatos guardados como un json\n",
    "import operator #Libreria para organizar de mayor a menor\n",
    "import matplotlib.pyplot as plt #libreria para graficas\n",
    "from nltk.corpus import stopwords, wordnet # importa las stop words y las palabras del ingles\n",
    "from nltk.stem.porter import PorterStemmer # metodo para stemming\n",
    "from nltk.stem.lancaster import LancasterStemmer # metodo para stemming\n",
    "from nltk.stem import WordNetLemmatizer # metodo para lematizar\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read Files\n",
    "#files_txt = glob.glob(\"/opt/datasets/mcda-pi1-20191/papers-txt/*.txt\")\n",
    "files_txt = glob.glob(\"C:\\\\Users\\\\Andres\\\\Desktop\\\\MAESTRIA\\\\SEMESTRE_I\\\\texto\\\\papers-txt\\\\*\") #Computer Andres\n",
    "\n",
    "# instanciar la clase para lematizar\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer() # instancia una forma de stemming\n",
    "# llamamos al diccionario de stop words en ingles\n",
    "sw = stopwords.words(\"english\")\n",
    "\n",
    "#leer Meta Datos\n",
    "meta_data = open(\"xml_parser/metadata_dict.txt\",\"r\",encoding='utf-8').read()\n",
    "meta_data = json.loads(meta_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(texto):\n",
    "    tokens = word_tokenize(texto.lower())\n",
    "    tokens = [re.sub('[^A-Za-z0-9]+','',word) for word in tokens if not bool(re.search(r'(.)\\1{2,}', word))]\n",
    "    return tokens\n",
    "\n",
    "def limpiar_tokens(tokens, is_metadata = False):\n",
    "    if is_metadata:\n",
    "        return [stemmer.stem(w.lower()) for w in tokens if (len(w)>1) and (w not in sw) and w.isalpha()]       \n",
    "    else: \n",
    "        return [stemmer.stem(w.lower()) for w in tokens if (len(w)>1) and (len(w)<15) and (w not in sw) and w.isalpha() ] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "documentos_text = list()\n",
    "documentos      = list()\n",
    "for file in files_txt:\n",
    "    #Leer Informacion\n",
    "    input_file = open(file,\"r\",encoding='utf-8')\n",
    "    texto = input_file.read()\n",
    "    meta_data_info = meta_data[file.replace(\"\\\\\",\"/\").split(\"/\")[-1].replace(\".txt\",\"\")]\n",
    "    \n",
    "    #tokenizacion\n",
    "    tokens = tokenizer(texto)\n",
    "    tokens_metada = tokenizer(meta_data_info)\n",
    "    #mirar cant\n",
    "    \n",
    "    # aplica lematizacion, stemming, elimina de stop words y aplica reglas lÃ³gicas para reducir la cantidad de tokens\n",
    "    tokens = limpiar_tokens(tokens)\n",
    "    tokens_metada = limpiar_tokens(tokens, is_metadata = True)\n",
    "    documentos_text.append(' '.join(tokens + tokens_metada))\n",
    "    documentos.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer(analyzer = \"word\",tokenizer = word_tokenize,    \\\n",
    "                             preprocessor = None, \\\n",
    "                             stop_words = stopwords.words(\"english\"),   \\\n",
    "                             lowercase = True,  \\\n",
    "                             max_features = None)\n",
    "\n",
    "\n",
    "bow_tf = count_vectorizer.fit_transform(documentos_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(980, 90211)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_tf.shape #doc, words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer=TfidfVectorizer(analyzer = \"word\",tokenizer = word_tokenize,    \\\n",
    "                             preprocessor = None, \\\n",
    "                             stop_words = stopwords.words(\"english\"),   \\\n",
    "                             lowercase = True,  \\\n",
    "                             max_features = None)\n",
    " \n",
    "# just send in all your docs here\n",
    "bow_tfidf = tfidf_vectorizer.fit_transform(documentos_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(980, 90211)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_tfidf.shape #doc, words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aa</th>\n",
       "      <th>aaa</th>\n",
       "      <th>aab</th>\n",
       "      <th>aaba</th>\n",
       "      <th>aabab</th>\n",
       "      <th>aababa</th>\n",
       "      <th>aababaababb</th>\n",
       "      <th>aabb</th>\n",
       "      <th>aabc</th>\n",
       "      <th>aabcdaccaac</th>\n",
       "      <th>...</th>\n",
       "      <th>zzf</th>\n",
       "      <th>zzi</th>\n",
       "      <th>zzmin</th>\n",
       "      <th>zzn</th>\n",
       "      <th>zzpe</th>\n",
       "      <th>zzr</th>\n",
       "      <th>zzth</th>\n",
       "      <th>zzw</th>\n",
       "      <th>zzwp</th>\n",
       "      <th>zzyzx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã 90211 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    aa  aaa  aab  aaba  aabab  aababa  aababaababb  aabb  aabc  aabcdaccaac  \\\n",
       "0  0.0  0.0  0.0   0.0    0.0     0.0          0.0   0.0   0.0          0.0   \n",
       "1  0.0  0.0  0.0   0.0    0.0     0.0          0.0   0.0   0.0          0.0   \n",
       "2  0.0  0.0  0.0   0.0    0.0     0.0          0.0   0.0   0.0          0.0   \n",
       "3  0.0  0.0  0.0   0.0    0.0     0.0          0.0   0.0   0.0          0.0   \n",
       "4  0.0  0.0  0.0   0.0    0.0     0.0          0.0   0.0   0.0          0.0   \n",
       "\n",
       "   ...    zzf  zzi  zzmin  zzn  zzpe  zzr  zzth  zzw  zzwp  zzyzx  \n",
       "0  ...    0.0  0.0    0.0  0.0   0.0  0.0   0.0  0.0   0.0    0.0  \n",
       "1  ...    0.0  0.0    0.0  0.0   0.0  0.0   0.0  0.0   0.0    0.0  \n",
       "2  ...    0.0  0.0    0.0  0.0   0.0  0.0   0.0  0.0   0.0    0.0  \n",
       "3  ...    0.0  0.0    0.0  0.0   0.0  0.0   0.0  0.0   0.0    0.0  \n",
       "4  ...    0.0  0.0    0.0  0.0   0.0  0.0   0.0  0.0   0.0    0.0  \n",
       "\n",
       "[5 rows x 90211 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_tfidf_df = pd.DataFrame(bow_tfidf.toarray(), columns=tfidf_vectorizer.get_feature_names())\n",
    "bow_tfidf_df.head()\n",
    "#bow_tfidf_df.loc[bow_tfidf_df['zzf'] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aa</th>\n",
       "      <th>aaa</th>\n",
       "      <th>aab</th>\n",
       "      <th>aaba</th>\n",
       "      <th>aabab</th>\n",
       "      <th>aababa</th>\n",
       "      <th>aababaababb</th>\n",
       "      <th>aabb</th>\n",
       "      <th>aabc</th>\n",
       "      <th>aabcdaccaac</th>\n",
       "      <th>...</th>\n",
       "      <th>zzf</th>\n",
       "      <th>zzi</th>\n",
       "      <th>zzmin</th>\n",
       "      <th>zzn</th>\n",
       "      <th>zzpe</th>\n",
       "      <th>zzr</th>\n",
       "      <th>zzth</th>\n",
       "      <th>zzw</th>\n",
       "      <th>zzwp</th>\n",
       "      <th>zzyzx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã 90211 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   aa  aaa  aab  aaba  aabab  aababa  aababaababb  aabb  aabc  aabcdaccaac  \\\n",
       "0   0    0    0     0      0       0            0     0     0            0   \n",
       "1   0    0    0     0      0       0            0     0     0            0   \n",
       "2   0    0    0     0      0       0            0     0     0            0   \n",
       "3   0    0    0     0      0       0            0     0     0            0   \n",
       "4   0    0    0     0      0       0            0     0     0            0   \n",
       "\n",
       "   ...    zzf  zzi  zzmin  zzn  zzpe  zzr  zzth  zzw  zzwp  zzyzx  \n",
       "0  ...      0    0      0    0     0    0     0    0     0      0  \n",
       "1  ...      0    0      0    0     0    0     0    0     0      0  \n",
       "2  ...      0    0      0    0     0    0     0    0     0      0  \n",
       "3  ...      0    0      0    0     0    0     0    0     0      0  \n",
       "4  ...      0    0      0    0     0    0     0    0     0      0  \n",
       "\n",
       "[5 rows x 90211 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_tf_df = pd.DataFrame(bow_tf.toarray(), columns=count_vectorizer.get_feature_names())\n",
    "bow_tf_df.head()\n",
    "#bow_tf_df.loc[bow_tf_df['aa'] > 0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "documentos_length = [sum(words) for words in bow_tf.toarray()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted_index = {}\n",
    "for word in bow_tfidf_df.columns:\n",
    "    inverted_index[word] = []\n",
    "    for row in bow_tfidf_df.loc[bow_tfidf_df[word] > 0].index:\n",
    "        doc = documentos[row]\n",
    "        tf = bow_tf_df[word][row]\n",
    "        tfidf = bow_tfidf_df[word][row]\n",
    "        doc_length = documentos_length[row]\n",
    "        inverted_index[word].append( (doc, tfidf, tf, doc_length) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_inverted_text(term):\n",
    "    resultado = []\n",
    "    for doc, freq,_,_ in inverted_index[term]:\n",
    "        resultado.append(doc)\n",
    "    return resultado\n",
    "len(query_inverted_text('biolog'))\n",
    "#inverted_index['characterist']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('C:\\\\Users\\\\Andres\\\\Desktop\\\\MAESTRIA\\\\SEMESTRE_I\\\\texto\\\\papers-txt\\\\1505.02348.txt',\n",
       "  0.36734786561867994),\n",
       " ('C:\\\\Users\\\\Andres\\\\Desktop\\\\MAESTRIA\\\\SEMESTRE_I\\\\texto\\\\papers-txt\\\\1506.00366.txt',\n",
       "  0.21268972254851137),\n",
       " ('C:\\\\Users\\\\Andres\\\\Desktop\\\\MAESTRIA\\\\SEMESTRE_I\\\\texto\\\\papers-txt\\\\1503.07759.txt',\n",
       "  0.14320533160016088),\n",
       " ('C:\\\\Users\\\\Andres\\\\Desktop\\\\MAESTRIA\\\\SEMESTRE_I\\\\texto\\\\papers-txt\\\\1412.0291.txt',\n",
       "  0.1268263615478158),\n",
       " ('C:\\\\Users\\\\Andres\\\\Desktop\\\\MAESTRIA\\\\SEMESTRE_I\\\\texto\\\\papers-txt\\\\1510.06482.txt',\n",
       "  0.10928431224996064),\n",
       " ('C:\\\\Users\\\\Andres\\\\Desktop\\\\MAESTRIA\\\\SEMESTRE_I\\\\texto\\\\papers-txt\\\\1505.05193.txt',\n",
       "  0.10645841968843538),\n",
       " ('C:\\\\Users\\\\Andres\\\\Desktop\\\\MAESTRIA\\\\SEMESTRE_I\\\\texto\\\\papers-txt\\\\1501.04836.txt',\n",
       "  0.06692920071518162),\n",
       " ('C:\\\\Users\\\\Andres\\\\Desktop\\\\MAESTRIA\\\\SEMESTRE_I\\\\texto\\\\papers-txt\\\\1504.06320.txt',\n",
       "  0.05219887602249112),\n",
       " ('C:\\\\Users\\\\Andres\\\\Desktop\\\\MAESTRIA\\\\SEMESTRE_I\\\\texto\\\\papers-txt\\\\1210.2246.txt',\n",
       "  0.04792221901826361),\n",
       " ('C:\\\\Users\\\\Andres\\\\Desktop\\\\MAESTRIA\\\\SEMESTRE_I\\\\texto\\\\papers-txt\\\\1403.1080.txt',\n",
       "  0.0449795754411322)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def query_inverted_text(terms=[]):\n",
    "    resultado = {}\n",
    "    for term in terms:\n",
    "        if term in inverted_index:\n",
    "            for doc, tfidf, tf, doc_length  in inverted_index[term]:\n",
    "                if doc in resultado:\n",
    "                    resultado[doc] += tfidf\n",
    "                else:\n",
    "                    resultado[doc] = tfidf\n",
    "    return resultado\n",
    "query = query_inverted_text(['biolog'])\n",
    "sorted_x = sorted(query.items(), key=operator.itemgetter(1), reverse=True)[:10]\n",
    "sorted_x\n",
    "#inverted_index['characterist']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['biolog']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('C:\\\\Users\\\\Andres\\\\Desktop\\\\MAESTRIA\\\\SEMESTRE_I\\\\texto\\\\papers-txt\\\\1505.02348.txt',\n",
       "  3.7846767326225335),\n",
       " ('C:\\\\Users\\\\Andres\\\\Desktop\\\\MAESTRIA\\\\SEMESTRE_I\\\\texto\\\\papers-txt\\\\1506.00366.txt',\n",
       "  3.781282745425505),\n",
       " ('C:\\\\Users\\\\Andres\\\\Desktop\\\\MAESTRIA\\\\SEMESTRE_I\\\\texto\\\\papers-txt\\\\1503.07759.txt',\n",
       "  3.7743498569931377),\n",
       " ('C:\\\\Users\\\\Andres\\\\Desktop\\\\MAESTRIA\\\\SEMESTRE_I\\\\texto\\\\papers-txt\\\\1412.0291.txt',\n",
       "  3.7495338385167765),\n",
       " ('C:\\\\Users\\\\Andres\\\\Desktop\\\\MAESTRIA\\\\SEMESTRE_I\\\\texto\\\\papers-txt\\\\1510.06482.txt',\n",
       "  3.7442781559319585),\n",
       " ('C:\\\\Users\\\\Andres\\\\Desktop\\\\MAESTRIA\\\\SEMESTRE_I\\\\texto\\\\papers-txt\\\\1505.05193.txt',\n",
       "  3.73523859759482),\n",
       " ('C:\\\\Users\\\\Andres\\\\Desktop\\\\MAESTRIA\\\\SEMESTRE_I\\\\texto\\\\papers-txt\\\\1504.06320.txt',\n",
       "  3.7045515853441846),\n",
       " ('C:\\\\Users\\\\Andres\\\\Desktop\\\\MAESTRIA\\\\SEMESTRE_I\\\\texto\\\\papers-txt\\\\1210.2246.txt',\n",
       "  3.666402408754918),\n",
       " ('C:\\\\Users\\\\Andres\\\\Desktop\\\\MAESTRIA\\\\SEMESTRE_I\\\\texto\\\\papers-txt\\\\1501.04836.txt',\n",
       "  3.6652192909136865),\n",
       " ('C:\\\\Users\\\\Andres\\\\Desktop\\\\MAESTRIA\\\\SEMESTRE_I\\\\texto\\\\papers-txt\\\\1105.1302.txt',\n",
       "  3.651464666810897)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def query_inverted_text_bm25(query, k1=1.2, b=0.75):\n",
    "    resultado = {}\n",
    "    query_tokens = tokenizer(query)\n",
    "    query_tokens = limpiar_tokens(query_tokens)\n",
    "    print(query_tokens)\n",
    "    avgdl = np.mean(documentos_length)\n",
    "    for term in query_tokens:\n",
    "        doc_query_conriene = len(inverted_index[term])\n",
    "        query_idf = np.log((len(documentos_length) - doc_query_conriene + 0.5) / (doc_query_conriene + 0.5))\n",
    "        for doc, tfidf, tf, doc_length  in inverted_index[term]:\n",
    "            upper = tf *  (k1 + 1)\n",
    "            below = tf + k1*(1 - b + (b * (doc_length / avgdl)))\n",
    "            if doc in resultado:\n",
    "                resultado[doc] += query_idf * (upper / below)\n",
    "            else:\n",
    "                resultado[doc] = query_idf * (upper / below)\n",
    "    return resultado\n",
    "query = query_inverted_text_bm25(\"biology\")\n",
    "sorted_x = sorted(query.items(), key=operator.itemgetter(1), reverse=True)[:10]\n",
    "sorted_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tfidf_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file1 = open(\"result_query.txt\",\"w\") \n",
    "file1.writelines('\\n'.join([doc for doc, tdfidf in sorted_x]))\n",
    "file1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(documentos[116])\n",
    "print(documentos[56])\n",
    "print(documentos[76])\n",
    "print(documentos[534])\n",
    "print(documentos[656])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
