{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text-mining-applied-project\n",
    "\n",
    "Búsqueda y Recuperación de Información \n",
    "Se busca construir un sistema de búsqueda y recuperación de información que sea capaz de identificar información relevante para el usuario en grandes volúmenes de información (Documentos Científicos). Este proceso se lleva a cabo mediante la construcción de un Bag of Words, método utilizado para representar la información de los documentos en palabras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob # libreria para extraer la ruta de los archivos\n",
    "import re # libreria para manejos de expresiones regulares\n",
    "import collections #### para poder contar los hash\n",
    "import pandas as pd # libreria para manejo de bases datos\n",
    "import numpy as np # libreria para manejo de vectores y arreglos\n",
    "import json #libreria para leer los metadatos guardados como un json\n",
    "import operator #Libreria para organizar de mayor a menor\n",
    "import matplotlib.pyplot as plt #libreria para graficas\n",
    "from nltk.corpus import stopwords, wordnet # importa las stop words y las palabras del ingles\n",
    "from nltk.stem.porter import PorterStemmer # metodo para stemming\n",
    "from nltk.stem.lancaster import LancasterStemmer # metodo para stemming\n",
    "from nltk.stem import WordNetLemmatizer # metodo para lematizar\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creación del Bag of Words\n",
    "Para la creación del Bag of Words se construye una lista de todos los documentos científicos almacenados en formato .txt, luego se extrae el texto de cada uno de los documentos y se realiza una limpieza para cada una de las palabras en los diferentes documentos. Entre los procesos de limpieza están:\n",
    "* Tokenización: Es el proceso de separar un texto en cadenas separadas por espacios o signos de puntuación.\n",
    "* Eliminar Stopwords: Eliminar palabras como artículos, conjunciones y preposiciones.\n",
    "* Stemming: Eliminar los sufijos de la palabra usando PorterStemmer de la librería NLTK.\n",
    "* Lematización: Proceso de convertir la palabra en la raíz\n",
    "* Otros: Limpieza de palabras de 1 carácter y palabras con más de tres letras consecutivas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read Files\n",
    "files_txt = glob.glob(\"/opt/datasets/mcda-pi1-20191/papers-txt/*.txt\")\n",
    "#files_txt = glob.glob(\"C:\\\\Users\\\\Andres\\\\Desktop\\\\MAESTRIA\\\\SEMESTRE_I\\\\texto\\\\papers-txt\\\\*\") #Computer Andres\n",
    "\n",
    "# instanciar la clase para lematizar\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer() # instancia una forma de stemming\n",
    "# llamamos al diccionario de stop words en ingles\n",
    "sw = stopwords.words(\"english\")\n",
    "\n",
    "#leer Meta Datos\n",
    "#meta_data = open(\"xml_parser/metadata_dict.txt\",\"r\",encoding='utf-8').read()\n",
    "#meta_data = json.loads(meta_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(texto):\n",
    "    texto  = texto.lower()\n",
    "    texto  = re.sub(\"\"\"(?:(?:https?|ftp):\\/\\/|\\b(?:[a-z\\d]+\\.))(?:(?:[^\\s()<>]+|\\((?:[^\\s()<>]+|(?:\\([^\\s()<>]+\\)))?\\))+(?:\\((?:[^\\s()<>]+|(?:\\(?:[^\\s()<>]+\\)))?\\)|[^\\s`!()\\[\\]{};:'\".,<>?«»“”‘’]))?\"\"\",' ',texto) # Eliminar las URL\n",
    "    texto  = re.sub('-|\\u2212|\\u2012|\\u2013|\\u2014|\\u2015',' ',texto)#\\(([^)]+)\\)|\\[([^)]+)\\]|\n",
    "    texto  = re.sub('((\\w*)?xyz|xyx|aba|abc|aab|zz|xx|yy|ijk(\\w*)?)', ' ',texto)\n",
    "    texto  = re.sub('[a-zA-Z0-9.?{}]+@\\w+\\.\\w+.\\w*',' ',texto) \n",
    "    texto  = re.sub('(á|à|ä)','a',texto)\n",
    "    texto  = re.sub('(é|è|ë)','e',texto)\n",
    "    texto  = re.sub('(í|ì|ï)','i',texto)\n",
    "    texto  = re.sub('(ó|ò|ö)','o',texto)\n",
    "    texto  = re.sub('(ú|ù|ü)','u',texto)\n",
    "    #texto = re.sub('[^A-Za-z0-9]+','',texto)\n",
    "    tokens = word_tokenize(texto)\n",
    "    tokens = [re.sub('[^A-Za-z0-9]+','',word) for word in tokens if not bool(re.search(r'(.)\\1{2,}', word))]\n",
    "    return tokens\n",
    "\n",
    "def limpiar_tokens(tokens, is_metadata = False):\n",
    "    if is_metadata:\n",
    "        #wordnet_lemmatizer.lemmatize(stemmer.stem(w.lower()), pos=\"v\")\n",
    "        return [wordnet_lemmatizer.lemmatize(stemmer.stem(w.lower()), pos=\"v\") for w in tokens if (len(w)>1) and (w not in sw) and w.isalpha()]       \n",
    "    else: \n",
    "        return [wordnet_lemmatizer.lemmatize(stemmer.stem(w.lower()), pos=\"v\") for w in tokens if (len(w)>1) and (len(w)<15) and (w not in sw) and w.isalpha() ] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "documentos_text = list()\n",
    "documentos      = list()\n",
    "for file in files_txt:\n",
    "    #Leer Informacion\n",
    "    input_file = open(file,\"r\",encoding='utf-8')\n",
    "    texto = input_file.read()\n",
    "    #meta_data_info = meta_data[file.replace(\"\\\\\",\"/\").split(\"/\")[-1].replace(\".txt\",\"\")]\n",
    "    \n",
    "    #tokenizacion\n",
    "    tokens = tokenizer(texto)\n",
    "    #tokens_metada = tokenizer(meta_data_info)\n",
    "    #mirar cant\n",
    "    \n",
    "    # aplica lematizacion, stemming, elimina de stop words y aplica reglas lógicas para reducir la cantidad de tokens\n",
    "    tokens = limpiar_tokens(tokens)\n",
    "    #tokens_metada = limpiar_tokens(tokens, is_metadata = True)\n",
    "    documentos_text.append(' '.join(tokens))# + tokens_metada\n",
    "    documentos.append(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Para la creación del Bag of Words\n",
    "\n",
    "### Usamos la librería de machine learning sklearn para construir nuestro diccionario de palabras \n",
    "\n",
    "* CountVectorizer: Nos permite contar el numero de palabras que ocurren en cada uno de los documentos tf\n",
    "* TfidfVectorizer: Calcula el tf  * idf donde idf = $log(\\frac{1+n}{1+df})+1$, cada fila del vector está en norma 2\n",
    "\n",
    "#### Referencias:\n",
    "1. [CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer)\n",
    "2. [TfidfVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'might', 'must', \"n't\", 'need', 'sha', 'wo', 'would'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "count_vectorizer = CountVectorizer(analyzer = \"word\",tokenizer = word_tokenize,    \\\n",
    "                             preprocessor = None, \\\n",
    "                             stop_words = sw,   \\\n",
    "                             lowercase = True,  \\\n",
    "                             max_features = None)\n",
    "\n",
    "\n",
    "bow_tf = count_vectorizer.fit_transform(documentos_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(980, 73643)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_tf.shape #doc, words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer=TfidfVectorizer(analyzer = \"word\",tokenizer = word_tokenize,    \\\n",
    "                             preprocessor = None, \\\n",
    "                             stop_words = sw,   \\\n",
    "                             lowercase = True,  \\\n",
    "                             max_features = None)\n",
    " \n",
    "# just send in all your docs here\n",
    "bow_tfidf = tfidf_vectorizer.fit_transform(documentos_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(980, 73643)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_tfidf.shape #doc, words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convertimos el resultado de CountVectorizer y TfidfVectorizer a un DataFrame de Pandas\n",
    "Este proceso fue necesario porque estos objetos utilizan las listas nativas de python y se hizo muy ineficiente al momento de construir el inverted index, ya que por ser una lista de listas este tenia que buscar sobre todo el conjunto de datos. Con pandas utilizando la función loc, podíamos buscar la palabra o retornando solo el tf que necesitábamos haciendo una búsqueda dentro de nuestro bag of words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aa</th>\n",
       "      <th>aaa</th>\n",
       "      <th>aab</th>\n",
       "      <th>aabb</th>\n",
       "      <th>aachen</th>\n",
       "      <th>aad</th>\n",
       "      <th>aadm</th>\n",
       "      <th>aaecc</th>\n",
       "      <th>aaem</th>\n",
       "      <th>aag</th>\n",
       "      <th>...</th>\n",
       "      <th>zytnicki</th>\n",
       "      <th>zyuban</th>\n",
       "      <th>zyvoloski</th>\n",
       "      <th>zyy</th>\n",
       "      <th>zz</th>\n",
       "      <th>zzf</th>\n",
       "      <th>zzi</th>\n",
       "      <th>zzn</th>\n",
       "      <th>zzpe</th>\n",
       "      <th>zzr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 73643 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    aa  aaa  aab  aabb  aachen  aad  aadm  aaecc  aaem  aag  ...  zytnicki  \\\n",
       "0  0.0  0.0  0.0   0.0     0.0  0.0   0.0    0.0   0.0  0.0  ...       0.0   \n",
       "1  0.0  0.0  0.0   0.0     0.0  0.0   0.0    0.0   0.0  0.0  ...       0.0   \n",
       "2  0.0  0.0  0.0   0.0     0.0  0.0   0.0    0.0   0.0  0.0  ...       0.0   \n",
       "3  0.0  0.0  0.0   0.0     0.0  0.0   0.0    0.0   0.0  0.0  ...       0.0   \n",
       "4  0.0  0.0  0.0   0.0     0.0  0.0   0.0    0.0   0.0  0.0  ...       0.0   \n",
       "\n",
       "   zyuban  zyvoloski  zyy   zz  zzf  zzi  zzn  zzpe  zzr  \n",
       "0     0.0        0.0  0.0  0.0  0.0  0.0  0.0   0.0  0.0  \n",
       "1     0.0        0.0  0.0  0.0  0.0  0.0  0.0   0.0  0.0  \n",
       "2     0.0        0.0  0.0  0.0  0.0  0.0  0.0   0.0  0.0  \n",
       "3     0.0        0.0  0.0  0.0  0.0  0.0  0.0   0.0  0.0  \n",
       "4     0.0        0.0  0.0  0.0  0.0  0.0  0.0   0.0  0.0  \n",
       "\n",
       "[5 rows x 73643 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_tfidf_df = pd.DataFrame(bow_tfidf.toarray(), columns=tfidf_vectorizer.get_feature_names())\n",
    "bow_tfidf_df.head()\n",
    "#bow_tfidf_df.loc[bow_tfidf_df['aa'] > 0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aa</th>\n",
       "      <th>aaa</th>\n",
       "      <th>aab</th>\n",
       "      <th>aabb</th>\n",
       "      <th>aachen</th>\n",
       "      <th>aad</th>\n",
       "      <th>aadm</th>\n",
       "      <th>aaecc</th>\n",
       "      <th>aaem</th>\n",
       "      <th>aag</th>\n",
       "      <th>...</th>\n",
       "      <th>zytnicki</th>\n",
       "      <th>zyuban</th>\n",
       "      <th>zyvoloski</th>\n",
       "      <th>zyy</th>\n",
       "      <th>zz</th>\n",
       "      <th>zzf</th>\n",
       "      <th>zzi</th>\n",
       "      <th>zzn</th>\n",
       "      <th>zzpe</th>\n",
       "      <th>zzr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 73643 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   aa  aaa  aab  aabb  aachen  aad  aadm  aaecc  aaem  aag  ...  zytnicki  \\\n",
       "0   0    0    0     0       0    0     0      0     0    0  ...         0   \n",
       "1   0    0    0     0       0    0     0      0     0    0  ...         0   \n",
       "2   0    0    0     0       0    0     0      0     0    0  ...         0   \n",
       "3   0    0    0     0       0    0     0      0     0    0  ...         0   \n",
       "4   0    0    0     0       0    0     0      0     0    0  ...         0   \n",
       "\n",
       "   zyuban  zyvoloski  zyy  zz  zzf  zzi  zzn  zzpe  zzr  \n",
       "0       0          0    0   0    0    0    0     0    0  \n",
       "1       0          0    0   0    0    0    0     0    0  \n",
       "2       0          0    0   0    0    0    0     0    0  \n",
       "3       0          0    0   0    0    0    0     0    0  \n",
       "4       0          0    0   0    0    0    0     0    0  \n",
       "\n",
       "[5 rows x 73643 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_tf_df = pd.DataFrame(bow_tf.toarray(), columns=count_vectorizer.get_feature_names())\n",
    "bow_tf_df.head()\n",
    "#bow_tf_df.loc[bow_tf_df['aa'] > 0].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inverted Index\n",
    "Es la estructura de datos más utilizada para la búsqueda en los sistemas de búsqueda y recuperación de información.\n",
    "Para la construcción de esta estructura de datos utilizamos los diccionarios de python para realizar una indexación por palabra y adicionar una lista de tuplas, donde cada una tiene la información del documento, el tfidf de la palabra, el tf y el tamaño en palabras del bag of words para el documento.\n",
    "\n",
    "##### Ejemplo:\n",
    "\n",
    "| word        | doc     | tfidf           | tf      | doc_length    |\n",
    "|-------------|---------|-----------------|---------|---------------|\n",
    "| biology     | [1,2,7] | [0.01,0.04,0.1] | [3,4,2] | [340,600,428] |\n",
    "| computer    | [2,3]   | [0.023,0.012]   | [5,18]  | [250,1118]    |\n",
    "| mathematics | [1]     | [0.1]           | [8]     | [128]         |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "documentos_length = [sum(words) for words in bow_tf.toarray()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted_index = {}\n",
    "for word in bow_tfidf_df.columns:\n",
    "    inverted_index[word] = []\n",
    "    for row in bow_tf_df.loc[bow_tf_df[word] > 0].index:\n",
    "        doc = documentos[row]\n",
    "        tf = bow_tf_df[word][row]\n",
    "        tfidf = bow_tfidf_df[word][row]\n",
    "        doc_length = documentos_length[row]\n",
    "        inverted_index[word].append( (doc, tfidf, tf, doc_length) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query\n",
    "Para realizar la búsqueda realizamos tres aproximaciones, ya que queríamos ir comparando los resultados poco a poco con el experto que utilizamos metapy, que es una librería en python utilizada para el procesamiento de texto.\n",
    "\n",
    "Utilizamos el Okapi BM25 que es una función de ranking que nos permite calificar las búsquedas para la Recuperación de información y asignar la relevancia a cada uno de los documentos en un buscador que tiene las palabras que el usuario desea buscar.\n",
    "\n",
    "\n",
    "$score(D,Q) = \\sum_{i=1}^{n}IDF(q_{i}) * \\frac{f(q_{i}, D) * (k_{1} + 1)}{f(q_{i}, D) + k_{1} * (1-b+b*\\frac{|D|}{avgdl})} $\n",
    "\n",
    "$k_{1}=1.2, b=0.75$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_inverted_text(term):\n",
    "    resultado = []\n",
    "    for doc, tfidf, tf, doc_length in inverted_index[term]:\n",
    "        resultado.append(doc)\n",
    "    return resultado\n",
    "len(query_inverted_text('biolog'))\n",
    "#inverted_index['characterist']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_inverted_text(terms=[]):\n",
    "    resultado = {}\n",
    "    for term in terms:\n",
    "        if term in inverted_index:\n",
    "            for doc, tfidf, tf, doc_length  in inverted_index[term]:\n",
    "                if doc in resultado:\n",
    "                    resultado[doc] += tfidf\n",
    "                else:\n",
    "                    resultado[doc] = tfidf\n",
    "    return resultado\n",
    "query = query_inverted_text(['biolog'])\n",
    "sorted_x = sorted(query.items(), key=operator.itemgetter(1), reverse=True)[:10]\n",
    "sorted_x\n",
    "#inverted_index['characterist']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('/opt/datasets/mcda-pi1-20191/papers-txt/1505.02348.txt', 3.744471187171522),\n",
       " ('/opt/datasets/mcda-pi1-20191/papers-txt/1506.00366.txt',\n",
       "  3.7355254483852254),\n",
       " ('/opt/datasets/mcda-pi1-20191/papers-txt/1503.07759.txt', 3.720408535682087),\n",
       " ('/opt/datasets/mcda-pi1-20191/papers-txt/1412.0291.txt', 3.680808436299234),\n",
       " ('/opt/datasets/mcda-pi1-20191/papers-txt/1510.06482.txt',\n",
       "  3.6630851349166123),\n",
       " ('/opt/datasets/mcda-pi1-20191/papers-txt/1505.05193.txt', 3.647304683707178),\n",
       " ('/opt/datasets/mcda-pi1-20191/papers-txt/1504.06320.txt',\n",
       "  3.5888082306740707),\n",
       " ('/opt/datasets/mcda-pi1-20191/papers-txt/1403.1080.txt', 3.540846238050834),\n",
       " ('/opt/datasets/mcda-pi1-20191/papers-txt/1210.2246.txt', 3.5199305499862357),\n",
       " ('/opt/datasets/mcda-pi1-20191/papers-txt/1501.04836.txt', 3.513527783313384)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def query_inverted_text_bm25(query, k1=1.2, b=0.75):\n",
    "    resultado = {}\n",
    "    query_tokens = tokenizer(query)\n",
    "    query_tokens = limpiar_tokens(query_tokens)\n",
    "    avgdl = np.mean(documentos_length)\n",
    "    for term in query_tokens:\n",
    "        doc_query_conriene = len(inverted_index[term])\n",
    "        query_idf = np.log((len(documentos_length) - doc_query_conriene + 0.5) / (doc_query_conriene + 0.5))\n",
    "        for doc, tfidf, tf, doc_length  in inverted_index[term]:\n",
    "            upper = tf *  (k1 + 1)\n",
    "            below = tf + k1*(1 - b + (b * (doc_length / avgdl)))\n",
    "            if doc in resultado:\n",
    "                resultado[doc] += query_idf * (upper / below)\n",
    "            else:\n",
    "                resultado[doc] = query_idf * (upper / below)\n",
    "    return resultado\n",
    "query = query_inverted_text_bm25(\"biology\")\n",
    "sorted_x = sorted(query.items(), key=operator.itemgetter(1), reverse=True)[:10]\n",
    "sorted_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MeTa\n",
    "Utilizaremos una librería de Python con herramienta para procesar texto y la utilizaremos para construir un índice invertido y generar un ranker que nos ayude a comparar los documentos recuperados por nuestra consulta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import metapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_idx = metapy.index.make_inverted_index(\"config.toml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "980"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv_idx.num_docs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranker = metapy.index.OkapiBM25()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_meta = metapy.index.Document()\n",
    "query_meta.content(\"statistics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_docs = ranker.score(inv_idx, query_meta, num_results=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(736, 1.7807728052139282),\n",
       " (658, 1.7700905799865723),\n",
       " (399, 1.7677022218704224),\n",
       " (16, 1.7660155296325684),\n",
       " (827, 1.7563282251358032),\n",
       " (677, 1.752448320388794),\n",
       " (805, 1.7521823644638062),\n",
       " (657, 1.7474273443222046),\n",
       " (34, 1.7392793893814087),\n",
       " (366, 1.7389888763427734)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('../data/papers_own_impl/1507.01279.txt', 1.7807728052139282),\n",
       " ('../data/papers_own_impl/1505.02214.txt', 1.7700905799865723),\n",
       " ('../data/papers_own_impl/1407.4908.txt', 1.7677022218704224),\n",
       " ('../data/papers_own_impl/0910.2912.txt', 1.7660155296325684),\n",
       " ('../data/papers_own_impl/1509.02900.txt', 1.7563282251358032),\n",
       " ('../data/papers_own_impl/1505.06770.txt', 1.752448320388794),\n",
       " ('../data/papers_own_impl/1508.04720.txt', 1.7521823644638062),\n",
       " ('../data/papers_own_impl/1505.02213.txt', 1.7474273443222046),\n",
       " ('../data/papers_own_impl/1006.1029.txt', 1.7392793893814087),\n",
       " ('../data/papers_own_impl/1405.4472.txt', 1.7389888763427734)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def meta_top_docs(top_docs):\n",
    "    person = []\n",
    "    for num, (d_id, d) in enumerate(top_docs):   \n",
    "        val_1 = inv_idx.label(d_id)\n",
    "        person.append((val_1, d))\n",
    "    return person\n",
    "person = meta_top_docs(top_docs)\n",
    "person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('/opt/datasets/mcda-pi1-20191/papers-txt/1507.01279.txt',\n",
       "  0.09782387519148865),\n",
       " ('/opt/datasets/mcda-pi1-20191/papers-txt/1407.4908.txt', 0.0974375625435099),\n",
       " ('/opt/datasets/mcda-pi1-20191/papers-txt/1505.02214.txt',\n",
       "  0.09741277700025769),\n",
       " ('/opt/datasets/mcda-pi1-20191/papers-txt/0910.2912.txt',\n",
       "  0.09725138772550106),\n",
       " ('/opt/datasets/mcda-pi1-20191/papers-txt/1311.4821.txt',\n",
       "  0.09702587201272124),\n",
       " ('/opt/datasets/mcda-pi1-20191/papers-txt/1508.04720.txt',\n",
       "  0.09693081884785062),\n",
       " ('/opt/datasets/mcda-pi1-20191/papers-txt/1109.2984.txt',\n",
       "  0.09686385499157207),\n",
       " ('/opt/datasets/mcda-pi1-20191/papers-txt/1505.02213.txt',\n",
       "  0.09680243646974188),\n",
       " ('/opt/datasets/mcda-pi1-20191/papers-txt/1509.02900.txt',\n",
       "  0.09675558129329453),\n",
       " ('/opt/datasets/mcda-pi1-20191/papers-txt/1505.06770.txt',\n",
       "  0.09626907788759705)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query2 = query_inverted_text_bm25(\"statistics\")\n",
    "sorted_x = sorted(query2.items(), key=operator.itemgetter(1), reverse=True)[:10]\n",
    "sorted_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#person[0][0].split(\"/\")[-1]\n",
    "y_true = [i[0].split('/')[-1] for i in person]\n",
    "y_pred = [j[0].split('/')[-1] for j in sorted_x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       "       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# matriz de confusion multiclase\n",
    "confusion_matrix(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "------------\n",
      "[1, 1, 1, 1, 0, 1, 0, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "y_trueb = [1 if x in y_true else 0 for x in y_true]\n",
    "print(y_trueb)\n",
    "print('------------')\n",
    "y_predb = [1 if x in y_true else 0 for x in y_pred]\n",
    "print(y_predb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 2, 8])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_true=y_trueb, y_pred=y_predb).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0, 2, 8)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tn, fp, fn, tp = confusion_matrix(y_true=y_trueb, y_pred=y_predb).ravel()\n",
    "(tn, fp, fn, tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 1.0)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "import matplotlib.pyplot as plt\n",
    "from inspect import signature\n",
    "\n",
    "precision, recall, _ = precision_recall_curve(y_trueb, y_predb)\n",
    "\n",
    "# In matplotlib < 1.5, plt.fill_between does not have a 'step' argument\n",
    "step_kwargs = ({'step': 'post'}\n",
    "               if 'step' in signature(plt.fill_between).parameters\n",
    "               else {})\n",
    "plt.step(recall, precision, color='b', alpha=0.2,\n",
    "         where='post')\n",
    "plt.fill_between(recall, precision, alpha=0.2, color='b', **step_kwargs)\n",
    "\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlim([0.0, 1.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision  1.0\n",
      "--------------------\n",
      "Recall  0.8\n"
     ]
    }
   ],
   "source": [
    "p = tp/(tp+fp)\n",
    "print('Precision ',round(p,2))\n",
    "print('--------------------')\n",
    "r = tp/(tp+fn)\n",
    "print('Recall ',round(r,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:32: RuntimeWarning: invalid value encountered in long_scalars\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'biology': (1.0, 1.0),\n",
       " 'activity': (1.0, 0.8),\n",
       " 'machine learning': (1.0, 0.8),\n",
       " 'machine': (1.0, 1.0),\n",
       " 'math': (1.0, 0.7),\n",
       " 'magazine': (1.0, 0.9),\n",
       " 'mahalanobis distance': (1.0, 1.0),\n",
       " 'kruskal algorithm': (1.0, 0.8),\n",
       " 'mathematician': (1.0, 0.9),\n",
       " 'norm': (1.0, 0.8),\n",
       " 'matrix norm': (1.0, 0.3),\n",
       " 'sparse': (1.0, 0.9),\n",
       " 'stochastic': (1.0, 0.9),\n",
       " 'news': (1.0, 0.7),\n",
       " 'random forest': (1.0, 0.1),\n",
       " 'logistic regression': (1.0, 0.9),\n",
       " 'statistics': (1.0, 0.8),\n",
       " 'root': (1.0, 1.0),\n",
       " 'python': (1.0, 1.0),\n",
       " 'inverse matrix': (1.0, 0.3),\n",
       " 'clustering': (1.0, 0.9),\n",
       " 'michael jordan': (1.0, 0.8),\n",
       " 'Carl Friedrich Gauss': (1.0, 0.8),\n",
       " 'europe': (1.0, 0.9),\n",
       " 'rank': (1.0, 0.9),\n",
       " 'Algebra': (1.0, 0.9),\n",
       " 'quick sort': (1.0, 0.5),\n",
       " 'education': (1.0, 0.9),\n",
       " 'Chinese rice': (1.0, 0.7),\n",
       " 'security': (1.0, 1.0),\n",
       " 'cryptography and network security': (1.0, 0.7),\n",
       " 'bank': (1.0, 0.9),\n",
       " 'radiology': (1.0, 1.0),\n",
       " 'google': (1.0, 0.9),\n",
       " 'page ranker': (1.0, 0.3),\n",
       " 'twitter': (1.0, 0.9),\n",
       " 'facebook': (1.0, 0.8),\n",
       " 'germany': (1.0, 0.7),\n",
       " 'colombia': (1.0, 0.83),\n",
       " 'linux': (1.0, 0.8),\n",
       " 'text': (1.0, 0.9),\n",
       " 'text mining': (1.0, 0.9),\n",
       " 'wikipedia': (1.0, 1.0),\n",
       " 'population': (1.0, 0.9),\n",
       " 'city': (1.0, 1.0),\n",
       " 'country': (1.0, 0.9),\n",
       " 'master and slave': (1.0, 0.8),\n",
       " 'Polynomial': (1.0, 0.9),\n",
       " 'Isaac Newton': (1.0, 0.8),\n",
       " 'ibm': (1.0, 0.8),\n",
       " 'red hat': (1.0, 0.8),\n",
       " 'Structured Query Language': (nan, 0.0),\n",
       " 'programming language sql': (1.0, 0.9),\n",
       " 'software': (1.0, 1.0),\n",
       " 'filesystem': (1.0, 1.0),\n",
       " 'albert einstein': (1.0, 0.9),\n",
       " 'android': (1.0, 1.0),\n",
       " 'gpu': (1.0, 1.0),\n",
       " 'cpu': (1.0, 0.8),\n",
       " 'keyboard': (1.0, 1.0),\n",
       " 'hardware': (1.0, 0.9),\n",
       " 'internet': (1.0, 0.9),\n",
       " 'intel': (1.0, 1.0),\n",
       " 'aws emr': (1.0, 0.9),\n",
       " 'magic spell': (1.0, 0.9),\n",
       " 'genetic algorithm': (1.0, 0.4),\n",
       " 'valuable gem': (1.0, 1.0),\n",
       " 'general electric': (1.0, 0.2),\n",
       " 'world clock': (1.0, 0.9),\n",
       " 'neural networks': (1.0, 0.7),\n",
       " 'big data': (nan, 0.0),\n",
       " 'bytes': (1.0, 1.0),\n",
       " 'tv': (1.0, 0.4),\n",
       " 'business analytics': (1.0, 1.0),\n",
       " 'mental disease': (1.0, 1.0),\n",
       " 'black market': (1.0, 0.8),\n",
       " 'integrator project': (1.0, 0.3),\n",
       " 'likelihood': (1.0, 0.9),\n",
       " 'text volume': (1.0, 0.7),\n",
       " 'amazon': (1.0, 0.8),\n",
       " 'Soft Condensed Matter': (1.0, 0.9),\n",
       " 'Software Engineering': (1.0, 0.8),\n",
       " 'Sound': (1.0, 1.0),\n",
       " 'Spectral Theory': (nan, 0.0),\n",
       " 'Statistical Finance': (1.0, 0.8),\n",
       " 'Statistical Mechanics': (1.0, 0.7)}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def truncate_round(n):\n",
    "    if np.isnan(n):\n",
    "        return np.nan\n",
    "    return int(n * 100) / 100\n",
    "\n",
    "def calificar_query(querys):\n",
    "    ranker = metapy.index.OkapiBM25()\n",
    "    resultado = {}\n",
    "    for query in querys:\n",
    "        query_meta = metapy.index.Document()\n",
    "        query_meta.content(query)\n",
    "        top_docs = ranker.score(inv_idx, query_meta, num_results=10)\n",
    "        expert = meta_top_docs(top_docs)\n",
    "        len_expert = len(expert)\n",
    "        \n",
    "        buscador = query_inverted_text_bm25(query)\n",
    "        buscador = sorted(buscador.items(), key=operator.itemgetter(1), reverse=True)[:len_expert]\n",
    "        \n",
    "        #buscador += [0 for i in range(len(expert) - len(buscador))]\n",
    "        y_true = [i[0].split('/')[-1] for i in expert]\n",
    "        y_pred = [j[0].split('/')[-1] for j in buscador]\n",
    "        #boolean form of y_true and y_pred\n",
    "        y_trueb = [1 if x in y_true else 0 for x in y_true]\n",
    "        y_predb = [1 if x in y_true else 0 for x in y_pred]\n",
    "        \n",
    "        conf_matrix = confusion_matrix(y_trueb, y_predb).ravel()\n",
    "        if len(conf_matrix) > 1:\n",
    "            tn, fp, fn, tp = conf_matrix\n",
    "        else:\n",
    "            tn, fp, fn = 0,0,0\n",
    "            tp = conf_matrix[0]\n",
    "        p = tp/(tp+fp) #print('Precision ',round(p,2))\n",
    "        r = tp/(tp+fn) #print('Recall ',round(r,2))\n",
    "        resultado[query] = (truncate_round(p), truncate_round(r))\n",
    "    return resultado\n",
    "resultado_compare = calificar_query([\"biology\", \"activity\", \"machine learning\",\n",
    "                 \"machine\", \"math\", \"magazine\", \"mahalanobis distance\",\n",
    "                 \"kruskal algorithm\", \"mathematician\", \"norm\", \"matrix norm\",\n",
    "                 \"sparse\", \"stochastic\", \"news\", \"random forest\", \"logistic regression\", \"statistics\",\n",
    "                 \"root\", \"python\", \"inverse matrix\", \"clustering\", \"michael jordan\", \"Carl Friedrich Gauss\",\n",
    "                 \"europe\", \"rank\",\"Algebra\", \"quick sort\", \"education\", \n",
    "                 \"Chinese rice\",\"security\",\"cryptography and network security\", \"bank\", \"radiology\",\n",
    "                 \"google\", \"page ranker\", \"twitter\", \"facebook\",\"germany\", \"colombia\", \"linux\", \"text\",\n",
    "                 \"text mining\", \"wikipedia\", \"population\", \"city\", \"country\", \"master and slave\",\n",
    "                 \"Polynomial\", \"Isaac Newton\", \"ibm\", \"red hat\", \"Structured Query Language\", \n",
    "                 \"programming language sql\", \"software\", \"filesystem\", \"albert einstein\", \"android\",\n",
    "                 \"gpu\", \"cpu\", \"keyboard\", \"hardware\", \"internet\", \"intel\",\"aws emr\", \"magic spell\", \n",
    "                 \"genetic algorithm\", \"valuable gem\",\"general electric\",\"world clock\",\"neural networks\",\n",
    "                 \"big data\", \"bytes\",\"tv\",\"business analytics\",\"mental disease\", \"black market\", \"integrator project\",\n",
    "                 \"likelihood\", \"text volume\", \"amazon\", \"Soft Condensed Matter\", \"Software Engineering\", \"Sound\", \n",
    "                 \"Spectral Theory\", \"Statistical Finance\", \"Statistical Mechanics\"])\n",
    "resultado_compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(count_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "querys_result = []\n",
    "values_result = []\n",
    "for key, values in resultado_compare.items():\n",
    "    querys_result.append(key)\n",
    "    values_result.append(values[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f5827f3e400>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.distplot(values_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.percentile(values_result, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Procesamiento de Metadatos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_datos = open(\"xml_parser/metadata_dict.txt\",\"r\",encoding='utf-8').read()\n",
    "meta_datos = json.loads(meta_datos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Smooth R\\\\'enyi Entropy of Ergodic Quantum Information Sources,We prove that the average smooth Renyi entropy rate will approach the entropy\\nrate of a stationary, ergodic information source, which is equal to the Shannon\\nentropy rate for a classical information source and the von Neumann entropy\\nrate for a quantum information source. Comment: 5 pages, no figures, ISIT 2007\""
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_datos['0704.3504']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "321"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documentos.index('/opt/datasets/mcda-pi1-20191/papers-txt/0704.3504.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "documentos_meta_datos = list()\n",
    "for file in files_txt:\n",
    "    #Leer Informacion\n",
    "    meta_datos_info = meta_datos[file.replace(\"\\\\\",\"/\").split(\"/\")[-1].replace(\".txt\",\"\")]\n",
    "    \n",
    "    #tokenizacion\n",
    "    tokens_meta_datos = tokenizer(meta_datos_info)\n",
    "    \n",
    "    # aplica lematizacion, stemming, elimina de stop words y aplica reglas lógicas para reducir la cantidad de tokens\n",
    "    tokens_meta_datos = limpiar_tokens(tokens_meta_datos, is_metadata = True)\n",
    "    documentos_meta_datos.append(' '.join(tokens_meta_datos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metadatos = pd.DataFrame(documentos_meta_datos,columns=['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>birdnest bayesian infer rate fraud detect revi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>salient object detect benchmark</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>convex recoveri interferometr measur</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>almost settl hard noncommut determin paper stu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>simpl algorithm comput bocp articl devis conci...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content\n",
       "0  birdnest bayesian infer rate fraud detect revi...\n",
       "1                    salient object detect benchmark\n",
       "2               convex recoveri interferometr measur\n",
       "3  almost settl hard noncommut determin paper stu...\n",
       "4  simpl algorithm comput bocp articl devis conci..."
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_metadatos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metadatos.to_csv(\"data/metadatos.csv\",sep=';',index=False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "# make sure pyspark tells workers to use python3 not 2 if both are installed\n",
    "os.environ['PYSPARK_PYTHON'] = '/opt/anaconda3/bin/python3'\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = '/opt/anaconda3/bin/ipython'\n",
    "conf = pyspark.SparkConf().setAppName('appName').setMaster('local')\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://hpcdis.dis.eafit.edu.co:4046\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>appName</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f5827b0b6d8>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.mllib.linalg import Vector, Vectors\n",
    "from pyspark.mllib.clustering import LDA, LDAModel\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer, CountVectorizer\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.linalg import Vectors, SparseVector\n",
    "from pyspark.ml.clustering import LDA, BisectingKMeans\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "import re\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "\n",
    "from pyspark.sql.types import StringType\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_meta_datos = spark.read.csv(\"data/metadatos.csv\", inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_meta_datos = dfs_meta_datos.withColumn(\"uid\", monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_text(record):\n",
    "    content = record[0]\n",
    "    words = content.split()\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "udf_cleantext = udf(cleanup_text , ArrayType(StringType()))\n",
    "clean_text = dfs_meta_datos.withColumn(\"words\", udf_cleantext(struct([dfs_meta_datos[x] for x in dfs_meta_datos.columns])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+--------------------+\n",
      "|             content|uid|               words|\n",
      "+--------------------+---+--------------------+\n",
      "|birdnest bayesian...|  0|[birdnest, bayesi...|\n",
      "|salient object de...|  1|[salient, object,...|\n",
      "|convex recoveri i...|  2|[convex, recoveri...|\n",
      "|almost settl hard...|  3|[almost, settl, h...|\n",
      "|simpl algorithm c...|  4|[simpl, algorithm...|\n",
      "|price polici sell...|  5|[price, polici, s...|\n",
      "|commut algorithm ...|  6|[commut, algorith...|\n",
      "|min cost travel s...|  7|[min, cost, trave...|\n",
      "|extens configur s...|  8|[extens, configur...|\n",
      "|complet simultan ...|  9|[complet, simulta...|\n",
      "+--------------------+---+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clean_text.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(inputCol=\"words\", outputCol=\"rawFeatures\", vocabSize = 1000)\n",
    "cvmodel = cv.fit(clean_text)\n",
    "featurizedData = cvmodel.transform(clean_text)\n",
    "\n",
    "vocab = cvmodel.vocabulary\n",
    "vocab_broadcast = sc.broadcast(vocab)\n",
    "\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "idfModel = idf.fit(featurizedData)\n",
    "rescaledData = idfModel.transform(featurizedData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+--------------------+--------------------+\n",
      "|             content|uid|               words|         rawFeatures|\n",
      "+--------------------+---+--------------------+--------------------+\n",
      "|birdnest bayesian...|  0|[birdnest, bayesi...|(1000,[0,1,2,3,4,...|\n",
      "|salient object de...|  1|[salient, object,...|(1000,[80,170,855...|\n",
      "|convex recoveri i...|  2|[convex, recoveri...|(1000,[51,193,349...|\n",
      "|almost settl hard...|  3|[almost, settl, h...|(1000,[0,1,4,7,8,...|\n",
      "|simpl algorithm c...|  4|[simpl, algorithm...|(1000,[0,1,3,4,7,...|\n",
      "|price polici sell...|  5|[price, polici, s...|(1000,[1,2,4,7,8,...|\n",
      "|commut algorithm ...|  6|[commut, algorith...|(1000,[0,1,4,8,14...|\n",
      "|min cost travel s...|  7|[min, cost, trave...|(1000,[0,1,2,7,8,...|\n",
      "|extens configur s...|  8|[extens, configur...|(1000,[47,227,838...|\n",
      "|complet simultan ...|  9|[complet, simulta...|(1000,[1,2,4,12,1...|\n",
      "+--------------------+---+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "featurizedData.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|         rawFeatures|\n",
      "+--------------------+\n",
      "|(1000,[0,1,2,3,4,...|\n",
      "|(1000,[80,170,855...|\n",
      "|(1000,[51,193,349...|\n",
      "|(1000,[0,1,4,7,8,...|\n",
      "|(1000,[0,1,3,4,7,...|\n",
      "|(1000,[1,2,4,7,8,...|\n",
      "|(1000,[0,1,4,8,14...|\n",
      "|(1000,[0,1,2,7,8,...|\n",
      "|(1000,[47,227,838...|\n",
      "|(1000,[1,2,4,12,1...|\n",
      "|(1000,[1,8,10,12,...|\n",
      "|(1000,[11,14,111,...|\n",
      "|(1000,[1,3,7,13,1...|\n",
      "|(1000,[1,2,8,10,1...|\n",
      "|(1000,[38,73,93,3...|\n",
      "|(1000,[1,3,5,7,25...|\n",
      "|(1000,[14,20,44,1...|\n",
      "|(1000,[6,131,136,...|\n",
      "|(1000,[1,2,3,4,7,...|\n",
      "|(1000,[1,3,8,10,1...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rescaledData.select('rawFeatures').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------------------------------------------------------------------------+\n",
      "|topic|topic_desc                                                                          |\n",
      "+-----+------------------------------------------------------------------------------------+\n",
      "|0    |[map, data, point, curv, loss, biolog, cross, algorithm, surfac, cell]              |\n",
      "|1    |[quantum, game, strategi, logic, classic, agent, mathcal, version, automata, lambda]|\n",
      "|2    |[power, delta, comput, size, parallel, total, independ, et, al, test]               |\n",
      "|3    |[graph, tree, color, edg, time, algorithm, problem, np, planar, vertic]             |\n",
      "|4    |[channel, bind, sourc, big, node, degre, relay, input, case, polynomi]              |\n",
      "|5    |[matric, group, random, block, error, rule, complex, entropi, distribut, correl]    |\n",
      "|6    |[protocol, receiv, attack, secur, messag, design, matrix, wireless, order, fix]     |\n",
      "|7    |[social, estim, detect, data, learn, network, decis, softwar, commun, test]         |\n",
      "|8    |[code, user, control, system, scheme, energi, video, inform, decod, interact]       |\n",
      "|9    |[method, model, imag, optim, network, nois, search, sampl, cost, learn]             |\n",
      "+-----+------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lda = LDA(k=10, seed=123, optimizer=\"em\", featuresCol=\"features\")\n",
    "\n",
    "ldamodel = lda.fit(rescaledData)\n",
    "\n",
    "ldatopics = ldamodel.describeTopics()\n",
    "\n",
    "def map_termID_to_Word(termIndices):\n",
    "    words = []\n",
    "    for termID in termIndices:\n",
    "        words.append(vocab_broadcast.value[termID])\n",
    "    \n",
    "    return words\n",
    "\n",
    "udf_map_termID_to_Word = udf(map_termID_to_Word , ArrayType(StringType()))\n",
    "ldatopics_mapped = ldatopics.withColumn(\"topic_desc\", udf_map_termID_to_Word(ldatopics.termIndices))\n",
    "ldatopics_mapped.select(ldatopics_mapped.topic, ldatopics_mapped.topic_desc).show(50,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldaResults = ldamodel.transform(rescaledData)\n",
    "ldaResults = ldaResults.select('uid','content','words','rawFeatures','features','topicDistribution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|uid|             content|               words|         rawFeatures|            features|   topicDistribution|\n",
      "+---+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|  0|birdnest bayesian...|[birdnest, bayesi...|(1000,[0,1,2,3,4,...|(1000,[0,1,2,3,4,...|[0.05606669340478...|\n",
      "|  1|salient object de...|[salient, object,...|(1000,[80,170,855...|(1000,[80,170,855...|[0.09601228764239...|\n",
      "|  2|convex recoveri i...|[convex, recoveri...|(1000,[51,193,349...|(1000,[51,193,349...|[0.08985809529357...|\n",
      "|  3|almost settl hard...|[almost, settl, h...|(1000,[0,1,4,7,8,...|(1000,[0,1,4,7,8,...|[0.02253886006069...|\n",
      "|  4|simpl algorithm c...|[simpl, algorithm...|(1000,[0,1,3,4,7,...|(1000,[0,1,3,4,7,...|[0.26679101942628...|\n",
      "|  5|price polici sell...|[price, polici, s...|(1000,[1,2,4,7,8,...|(1000,[1,2,4,7,8,...|[0.05068687522568...|\n",
      "|  6|commut algorithm ...|[commut, algorith...|(1000,[0,1,4,8,14...|(1000,[0,1,4,8,14...|[0.07632761914073...|\n",
      "|  7|min cost travel s...|[min, cost, trave...|(1000,[0,1,2,7,8,...|(1000,[0,1,2,7,8,...|[0.02766751400927...|\n",
      "|  8|extens configur s...|[extens, configur...|(1000,[47,227,838...|(1000,[47,227,838...|[0.10733910486917...|\n",
      "|  9|complet simultan ...|[complet, simulta...|(1000,[1,2,4,12,1...|(1000,[1,2,4,12,1...|[0.04654012977584...|\n",
      "| 10|fix ingredi alrea...|[fix, ingredi, al...|(1000,[1,8,10,12,...|(1000,[1,8,10,12,...|[0.04332105733342...|\n",
      "| 11|taxat polici maxi...|[taxat, polici, m...|(1000,[11,14,111,...|(1000,[11,14,111,...|[0.08808509768980...|\n",
      "| 12|quantum lyapunov ...|[quantum, lyapuno...|(1000,[1,3,7,13,1...|(1000,[1,3,7,13,1...|[0.03557622172727...|\n",
      "| 13|iter receiv ofdm ...|[iter, receiv, of...|(1000,[1,2,8,10,1...|(1000,[1,2,8,10,1...|[0.02261787887227...|\n",
      "| 14|semidefinit progr...|[semidefinit, pro...|(1000,[38,73,93,3...|(1000,[38,73,93,3...|[0.08273446988385...|\n",
      "| 15|overview nomin ty...|[overview, nomin,...|(1000,[1,3,5,7,25...|(1000,[1,3,5,7,25...|[0.05315987394233...|\n",
      "| 16|bite recycl scale...|[bite, recycl, sc...|(1000,[14,20,44,1...|(1000,[14,20,44,1...|[0.10656355919600...|\n",
      "| 17|semi algebra colo...|[semi, algebra, c...|(1000,[6,131,136,...|(1000,[6,131,136,...|[0.08709307087108...|\n",
      "| 18|use isabel verifi...|[use, isabel, ver...|(1000,[1,2,3,4,7,...|(1000,[1,2,3,4,7,...|[0.07031184781338...|\n",
      "| 19|captur ari existe...|[captur, ari, exi...|(1000,[1,3,8,10,1...|(1000,[1,3,8,10,1...|[0.03625534842831...|\n",
      "+---+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ldaResults.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_topic_udf(topicDistribution):\n",
    "    dom = topicDistribution[0]\n",
    "    index_dom = 0\n",
    "    for index in range(len(topicDistribution)):\n",
    "        if (topicDistribution[index]>dom):\n",
    "            dom=topicDistribution[index]\n",
    "            index_dom=index\n",
    "    \n",
    "    return index_dom\n",
    "\n",
    "udf_seltop = udf(select_topic_udf , IntegerType())\n",
    "ldaResults = ldaResults.withColumn(\"topic_prin\", udf_seltop(ldaResults.topicDistribution))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|uid|             content|               words|         rawFeatures|            features|   topicDistribution|topic_prin|\n",
      "+---+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|  0|birdnest bayesian...|[birdnest, bayesi...|(1000,[0,1,2,3,4,...|(1000,[0,1,2,3,4,...|[0.05606669340478...|         9|\n",
      "|  1|salient object de...|[salient, object,...|(1000,[80,170,855...|(1000,[80,170,855...|[0.09601228764239...|         7|\n",
      "|  2|convex recoveri i...|[convex, recoveri...|(1000,[51,193,349...|(1000,[51,193,349...|[0.08985809529357...|         9|\n",
      "|  3|almost settl hard...|[almost, settl, h...|(1000,[0,1,4,7,8,...|(1000,[0,1,4,7,8,...|[0.02253886006069...|         5|\n",
      "|  4|simpl algorithm c...|[simpl, algorithm...|(1000,[0,1,3,4,7,...|(1000,[0,1,3,4,7,...|[0.26679101942628...|         0|\n",
      "|  5|price polici sell...|[price, polici, s...|(1000,[1,2,4,7,8,...|(1000,[1,2,4,7,8,...|[0.05068687522568...|         1|\n",
      "|  6|commut algorithm ...|[commut, algorith...|(1000,[0,1,4,8,14...|(1000,[0,1,4,8,14...|[0.07632761914073...|         5|\n",
      "|  7|min cost travel s...|[min, cost, trave...|(1000,[0,1,2,7,8,...|(1000,[0,1,2,7,8,...|[0.02766751400927...|         9|\n",
      "|  8|extens configur s...|[extens, configur...|(1000,[47,227,838...|(1000,[47,227,838...|[0.10733910486917...|         7|\n",
      "|  9|complet simultan ...|[complet, simulta...|(1000,[1,2,4,12,1...|(1000,[1,2,4,12,1...|[0.04654012977584...|         5|\n",
      "+---+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ldaResults.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['channel',\n",
       " 'bind',\n",
       " 'sourc',\n",
       " 'big',\n",
       " 'node',\n",
       " 'degre',\n",
       " 'relay',\n",
       " 'input',\n",
       " 'case',\n",
       " 'polynomi']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ldatopics_mapped.where((col(\"topic\") == 4)).select('topic_desc').collect()[0]['topic_desc']\n",
    "#.select('Category').collect()[0]['Category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>content</th>\n",
       "      <th>words</th>\n",
       "      <th>rawFeatures</th>\n",
       "      <th>features</th>\n",
       "      <th>topicDistribution</th>\n",
       "      <th>topic_prin</th>\n",
       "      <th>maximo</th>\n",
       "      <th>doc_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>birdnest bayesian infer rate fraud detect revi...</td>\n",
       "      <td>[birdnest, bayesian, infer, rate, fraud, detec...</td>\n",
       "      <td>(2.0, 1.0, 1.0, 1.0, 1.0, 4.0, 1.0, 1.0, 1.0, ...</td>\n",
       "      <td>(2.686790030171601, 0.39936752824004584, 1.236...</td>\n",
       "      <td>[0.05606669340478734, 0.027914667227744303, 0....</td>\n",
       "      <td>9</td>\n",
       "      <td>0.334765</td>\n",
       "      <td>1511.06030.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>salient object detect benchmark</td>\n",
       "      <td>[salient, object, detect, benchmark]</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.09601228764239991, 0.08818783429711856, 0.0...</td>\n",
       "      <td>7</td>\n",
       "      <td>0.168919</td>\n",
       "      <td>1501.02741.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>convex recoveri interferometr measur</td>\n",
       "      <td>[convex, recoveri, interferometr, measur]</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.08985809529357405, 0.08799815889284404, 0.0...</td>\n",
       "      <td>9</td>\n",
       "      <td>0.153053</td>\n",
       "      <td>1307.6864.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>almost settl hard noncommut determin paper stu...</td>\n",
       "      <td>[almost, settl, hard, noncommut, determin, pap...</td>\n",
       "      <td>(1.0, 1.0, 0.0, 0.0, 10.0, 0.0, 0.0, 1.0, 1.0,...</td>\n",
       "      <td>(1.3433950150858005, 0.39936752824004584, 0.0,...</td>\n",
       "      <td>[0.022538860060691522, 0.023640138650151934, 0...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.782061</td>\n",
       "      <td>1101.1169.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>simpl algorithm comput bocp articl devis conci...</td>\n",
       "      <td>[simpl, algorithm, comput, bocp, articl, devis...</td>\n",
       "      <td>(4.0, 1.0, 0.0, 1.0, 2.0, 0.0, 0.0, 1.0, 0.0, ...</td>\n",
       "      <td>(5.373580060343202, 0.39936752824004584, 0.0, ...</td>\n",
       "      <td>[0.26679101942628947, 0.05200593777769224, 0.1...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.266791</td>\n",
       "      <td>1211.0729.txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   uid                                            content  \\\n",
       "0    0  birdnest bayesian infer rate fraud detect revi...   \n",
       "1    1                    salient object detect benchmark   \n",
       "2    2               convex recoveri interferometr measur   \n",
       "3    3  almost settl hard noncommut determin paper stu...   \n",
       "4    4  simpl algorithm comput bocp articl devis conci...   \n",
       "\n",
       "                                               words  \\\n",
       "0  [birdnest, bayesian, infer, rate, fraud, detec...   \n",
       "1               [salient, object, detect, benchmark]   \n",
       "2          [convex, recoveri, interferometr, measur]   \n",
       "3  [almost, settl, hard, noncommut, determin, pap...   \n",
       "4  [simpl, algorithm, comput, bocp, articl, devis...   \n",
       "\n",
       "                                         rawFeatures  \\\n",
       "0  (2.0, 1.0, 1.0, 1.0, 1.0, 4.0, 1.0, 1.0, 1.0, ...   \n",
       "1  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "2  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3  (1.0, 1.0, 0.0, 0.0, 10.0, 0.0, 0.0, 1.0, 1.0,...   \n",
       "4  (4.0, 1.0, 0.0, 1.0, 2.0, 0.0, 0.0, 1.0, 0.0, ...   \n",
       "\n",
       "                                            features  \\\n",
       "0  (2.686790030171601, 0.39936752824004584, 1.236...   \n",
       "1  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "2  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3  (1.3433950150858005, 0.39936752824004584, 0.0,...   \n",
       "4  (5.373580060343202, 0.39936752824004584, 0.0, ...   \n",
       "\n",
       "                                   topicDistribution  topic_prin    maximo  \\\n",
       "0  [0.05606669340478734, 0.027914667227744303, 0....           9  0.334765   \n",
       "1  [0.09601228764239991, 0.08818783429711856, 0.0...           7  0.168919   \n",
       "2  [0.08985809529357405, 0.08799815889284404, 0.0...           9  0.153053   \n",
       "3  [0.022538860060691522, 0.023640138650151934, 0...           5  0.782061   \n",
       "4  [0.26679101942628947, 0.05200593777769224, 0.1...           0  0.266791   \n",
       "\n",
       "         doc_name  \n",
       "0  1511.06030.txt  \n",
       "1  1501.02741.txt  \n",
       "2   1307.6864.txt  \n",
       "3   1101.1169.txt  \n",
       "4   1211.0729.txt  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pd = ldaResults.toPandas()\n",
    "df_pd['maximo'] = df_pd['topicDistribution'].apply(lambda x: x.max())\n",
    "df_pd['doc_name']    = df_pd['uid'].apply(lambda x: documentos[x].replace(\"\\\\\",\"/\").split(\"/\")[-1])\n",
    "df_pd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_txt = \"biology\"\n",
    "query = query_inverted_text_bm25(\"biology\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se presentan 10 resultados para la palabra 'biology'\n",
      "             \n",
      "El documento mas relevante es 1505.02348.txt\n",
      "             \n",
      "El documento mas relevante pertenece al topico 0 este se describe con las palabras 'map, data, point, curv, loss, biolog, cross, algorithm, surfac, cell'             \n",
      "En el resultado del bm25 tenemos 5 documentos que pertenecen al mismo topico y de los 980 papers tenemos 94 que pertenecen a la misma categoria\n",
      "             \n",
      "Los documentos mas relevantes segun topicos son: \n",
      "- 1509.05821.txt\n",
      "- 1503.07759.txt\n",
      "- 1506.04391.txt\n",
      "- 1505.04911.txt\n",
      "- 1310.6324.txt\n",
      "          \n"
     ]
    }
   ],
   "source": [
    "def topicos_query(query, query_txt):\n",
    "    resultados = sorted(query_result.items(), key=operator.itemgetter(1), reverse=True)[:10]\n",
    "    mejor_resultado = resultados[0]\n",
    "    mejor_doc = mejor_resultado[0]\n",
    "    mejor_doc_clean = mejor_doc.replace(\"\\\\\",\"/\").split(\"/\")[-1]\n",
    "    score     = mejor_resultado[1]\n",
    "    index_doc = documentos.index(mejor_doc)\n",
    "    topico = int(df_pd.iloc[[index_doc]]['topic_prin'])\n",
    "    topic_desc = ldatopics_mapped.where((col(\"topic\") == topico)).select('topic_desc').collect()[0]['topic_desc']\n",
    "    doc_mismo_topico = list(df_pd.loc[df_pd['topic_prin'] == topico].sort_values('maximo',ascending=0)['doc_name'])\n",
    "    doc_mismo_topico.remove(mejor_doc_clean)\n",
    "    resultado_topico_counter = 0\n",
    "    for resultado in resultados[1:]:\n",
    "        resultado_doc = resultado[0]\n",
    "        resultado_index_doc = documentos.index(resultado_doc)\n",
    "        resultado_topico = int(df_pd.iloc[[resultado_index_doc]]['topic_prin'])\n",
    "        if topico == resultado_topico:\n",
    "            resultado_topico_counter += 1\n",
    "    print(\"\"\"Se presentan {} resultados para la palabra '{}'\n",
    "             \\nEl documento mas relevante es {}\n",
    "             \\nEl documento mas relevante pertenece al topico {} este se describe con las palabras '{}'\\\n",
    "             \\nEn el resultado del bm25 tenemos {} documentos que pertenecen al mismo topico y de los {} papers tenemos {} que pertenecen a la misma categoria\n",
    "             \\nLos documentos mas relevantes segun topicos son: \\n- {}\n",
    "          \"\"\".format(len(resultados),query_txt,mejor_doc_clean, topico, \", \".join(topic_desc), resultado_topico_counter, len(documentos), len(doc_mismo_topico),\n",
    "                     '\\n- '.join(doc_mismo_topico[:5])))\n",
    "topicos_query(query, query_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se presentan 10 resultados para la palabra 'graph'\n",
      "             \n",
      "El documento mas relevante es 1502.05183.txt\n",
      "             \n",
      "El documento mas relevante pertenece al topico 8 este se describe con las palabras 'code, user, control, system, scheme, energi, video, inform, decod, interact'             \n",
      "En el resultado del bm25 tenemos 0 documentos que pertenecen al mismo topico y de los 980 papers tenemos 90 que pertenecen a la misma categoria\n",
      "             \n",
      "Los documentos mas relevantes segun topicos son: \n",
      "- 1204.1868.txt\n",
      "- 1212.1710.txt\n",
      "- 1204.6321.txt\n",
      "- 1302.5906.txt\n",
      "- 1307.0449.txt\n",
      "          \n"
     ]
    }
   ],
   "source": [
    "query_txt = \"graph\"\n",
    "query_result = query_inverted_text_bm25(query_txt)\n",
    "topicos_query(query_result, query_txt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.6(conda)",
   "language": "python",
   "name": "python36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
