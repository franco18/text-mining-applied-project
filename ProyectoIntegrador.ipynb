{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text-mining-applied-project\n",
    "### Busqueda y Recuperacion de Informacion\n",
    "Se busca construir un sistema de busqueda y recuperacion de informacion que sea caàz de identificar informacion relavante para el usuario en grandes volumnes de informacion(Documentos Cientificos). Este proceso se lleva a cabo mediante la construccion de un Bag of Words metodo utilizado para representar la informacion de los documentos en palabras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob # libreria para extraer la ruta de los archivos\n",
    "import re # libreria para manejos de expresiones regulares\n",
    "import collections #### para poder contar los hash\n",
    "import pandas as pd # libreria para manejo de bases datos\n",
    "import numpy as np # libreria para manejo de vectores y arreglos\n",
    "import json #libreria para leer los metadatos guardados como un json\n",
    "import operator #Libreria para organizar de mayor a menor\n",
    "from nltk.corpus import stopwords, wordnet # importa las stop words y las palabras del ingles\n",
    "from nltk.stem.porter import PorterStemmer # metodo para stemming\n",
    "from nltk.stem.lancaster import LancasterStemmer # metodo para stemming\n",
    "from nltk.stem import WordNetLemmatizer # metodo para lematizar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creacion del Bag of Words\n",
    "Para la creacion del Bag of Words se construye una lista de todos los documentos cientificos almacenados en formato txt, luego se extrae el texto de cada uno de los documentos y se realiza una limpieza para cada una de las palabras en los diferentes documentos. Entre los procesos de limpieza estan:\n",
    "* Tokenización: Es el proceso de separa un texto en cadenas separadas por espacios o signos de puntuacion.\n",
    "* Eliminar Stopwords: Eliminar palabras como artículos, conjunciones y preposiciones.\n",
    "* Stemming: Eliminar los sufijos de la palabra usando PorterStemmer dela libreria NLTK.\n",
    "* Lemmatization: Proceso de convertir la palabra en la raiz\n",
    "* Otros: Limpieza de palabras de 1 caracter y palabras pertenecientes al ingles\n",
    "\n",
    "La estructura de datos que almancena el Bag of Words es ```[:documento][:palabra][:propiedades] ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read Files\n",
    "files_txt = glob.glob(\"/opt/datasets/mcda-pi1-20191/papers-txt/*.txt\")\n",
    "\n",
    "# instanciar la clase para lematizar\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "# llamamos al diccionario de stop words en ingles\n",
    "sw = stopwords.words(\"english\")\n",
    "\n",
    "#leer Meta Datos\n",
    "meta_data = open(\"xml_parser/metadata_dict.txt\",\"r\",encoding='utf-8').read()\n",
    "meta_data = json.loads(meta_data)\n",
    "\n",
    "# Recorre los archivos para generar el bag of words\n",
    "bag_of_words = {} # creacion de la estructura de datos (diccionario) para almacenar el bag of words\n",
    "for file in files_txt:\n",
    "    #Leer Informacion\n",
    "    input_file = open(file,\"r\",encoding='utf-8')\n",
    "    texto = input_file.read()\n",
    "    meta_data_info = meta_data[file.split(\"/\")[5].replace(\".txt\",\"\")]\n",
    "    \n",
    "    #limpieza de palabras\n",
    "    texto = re.sub('[^A-Za-z0-9]+',' ',texto) # solo permanecen los elementos alfanumericos\n",
    "    tokens = texto.split() # Realiza la tokenizacion\n",
    "    \n",
    "    meta_data_info = re.sub('[^A-Za-z0-9]+',' ',meta_data_info) # solo permanecen los elementos alfanumericos\n",
    "    tokens_metada = meta_data_info.split() # Realiza la tokenizacion\n",
    "    \n",
    "    stemmer = PorterStemmer() # instancia una forma de stemming\n",
    "    # aplica lematizacion, stemming, elimina de stop words y aplica reglas logicas para reducir la cantidad de tokens\n",
    "    tokens = [wordnet_lemmatizer.lemmatize(stemmer.stem(w.lower()), pos=\"v\") for w in tokens if (len(w)>1) and (w not in sw) and wordnet.synsets(w) and w.isalpha() ] # Longitud mayor a 1\n",
    "    tokens_metada = [wordnet_lemmatizer.lemmatize(stemmer.stem(w.lower()), pos=\"v\") for w in tokens if (len(w)>1) and (w not in sw)  ]\n",
    "    \n",
    "    # realiza el conteo de la frecuencia de cada palabra para cada documento\n",
    "    counter = collections.Counter(tokens + tokens_metada)\n",
    "    # para un diccionario almacenamos el key que es el nombre del documento y el value que es un diccionario con las key como palabras y los value como la frecuencia de la palabra\n",
    "    bag_of_words[file] = dict(counter)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelos\n",
    "Dentro de los métodos para recuperar información de los textos, existen varios tipos de modelos. Por el momento, los modelos que están implementados para la recuperación de información son:\n",
    "1.\tTerm Frequency (TF): Es la técnica más simple para reconocer la relevancia de un término dentro de un texto. Básicamente realiza el conteo de la palabra en el texto y mientras más grande sea este número más relevante es\n",
    "2.\tRelative Term Frequency (RTF): Está técnica vuelve relativo al número de palabras total el conteo anterior, representando entonces cuánto porcentaje del texto está explicado por esa palabra\n",
    "3.\tT-Term Frequency (T-TF): Para eliminar riesgos de modelo cuando se realiza el conteo lineal de la frecuencia de los términos, se propone trabajar con una transformación del conteo: T-TF = 1 + log(x) de esta manera no se benefician aquellas palabras que aparecen muchas veces en un documento, pues no necesariamente son más relevantes que las demás\n",
    "4.\tInverse Document Frequency (IDF): Este modelo está basado en el principio de que mientras menor sea la frecuencia de la palabra en el documento, más relevante y más información puede tener IDF = log(# total de documentos/# de documentos donde está la palabra)\n",
    "5.\tTF-IDF: Al tener los modelos ya cuantificados, tanto el TF (recomendable trabajar con la transformación) y el IDF, la multiplicación de ambos entrega información valiosa de cara a la similaridad de la búsqueda o Query con el documento.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#contar cada palabra en cuantos documentos se encuentra\n",
    "helper = {}\n",
    "for doc, words in bag_of_words.items():\n",
    "    for word in words.keys():\n",
    "        if word in helper:\n",
    "            helper[word] += 1\n",
    "        else:\n",
    "            helper[word] = 1\n",
    "\n",
    "#Para las palabras de cada documento asignar medidas\n",
    "for documento, words in bag_of_words.items():\n",
    "    num_words = sum(bag_of_words[documento].values())\n",
    "    for key, value in words.items():\n",
    "        dic = {}\n",
    "        # Asignar medidas\n",
    "        dic['freq'] = value # TF\n",
    "        dic['freqR'] = value / num_words # RTF\n",
    "        dic['tf'] = 1 + np.log(value) # T-TF\n",
    "        dic['idf'] = np.log(len(bag_of_words) / helper[key]) # IDF\n",
    "        dic['tf-idf'] = dic['tf'] * dic['idf'] # TF-IDF\n",
    "        bag_of_words[documento][key] = dic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'freq': 170,\n",
       " 'freqR': 0.022966765738989463,\n",
       " 'idf': 1.0704414117014134,\n",
       " 'tf': 6.135798437050262,\n",
       " 'tf-idf': 6.568012740871408}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_of_words['/opt/datasets/mcda-pi1-20191/papers-txt/1508.04417.txt']['user']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Primer acercamiento a Busquedas \n",
    "Se debe realizar el mismo proceso de limpieza al query ya que con este se van a buscar las palabras en el Bag of Words para buscar y identificar los textos mas relevantes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'averag': 1, 'good': 1, 'move': 1, 'trade': 2}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#query\n",
    "def transformar_informacion (texto):\n",
    "    texto = re.sub('[^A-Za-z0-9]+',' ',texto) # Caracteres especiales\n",
    "    texto = texto.replace('í','i')\n",
    "    tokens = texto.split()\n",
    "    stemmer = PorterStemmer()\n",
    "    stemmer2 = LancasterStemmer()\n",
    "    tokens = [wordnet_lemmatizer.lemmatize(stemmer.stem(w.lower()), pos=\"v\") for w in tokens if (len(w)>1) and w.isalpha() and w not in sw and wordnet.synsets(w)] # Longitud mayor a 1\n",
    "    counter=collections.Counter(tokens)\n",
    "    return dict(counter)\n",
    "\n",
    "texto = \"Moving average are not good for trading\"\n",
    "query = transformar_informacion(texto)\n",
    "query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Realizar Busequeda\n",
    "Despues de realizar la transformacion del query utilizando los mismos proceso de limpieza que el Bag of Words, se busca cada palabra en el query para cada documento y se calcula ```[:palabra_query][:freq] * [:documento][:palabra][:tf]```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Son 813 resultados.... [('/opt/datasets/mcda-pi1-20191/papers-txt/1106.1445.txt', 28.45588170404503), ('/opt/datasets/mcda-pi1-20191/papers-txt/1110.2053.txt', 20.43861466116219), ('/opt/datasets/mcda-pi1-20191/papers-txt/1505.02214.txt', 19.482631546578965), ('/opt/datasets/mcda-pi1-20191/papers-txt/1508.03891.txt', 18.355530128203853), ('/opt/datasets/mcda-pi1-20191/papers-txt/1411.4097.txt', 18.25583076798076)]\n"
     ]
    }
   ],
   "source": [
    "result_query = {}\n",
    "for documento, words in bag_of_words.items():\n",
    "    for key, value in words.items():\n",
    "        if key in query:\n",
    "            if documento in result_query:\n",
    "                result_query[documento] += query[key] * value['tf']\n",
    "            else:\n",
    "                result_query[documento] = query[key] * value['tf']\n",
    "\n",
    "                \n",
    "resultado = sorted(result_query.items(), key=operator.itemgetter(1), reverse=True)\n",
    "print(\"Son {} resultados.... {}\".format(len(resultado), resultado[:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for documento, words in bag_of_words.items():\n",
    "    for key, value in words.items():\n",
    "        print(value)\n",
    "        break\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Contar documentos\n",
    "len(bag_of_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Contar numero de palabras por documento despues de limpieza\n",
    "x_doc = [] # documentos\n",
    "y_doc = [] # num palabras unicas por doc\n",
    "total_palabras_doc = [] # total palabras\n",
    "for documento, words in bag_of_words.items():\n",
    "    x_doc.append(documento)\n",
    "    y_doc.append(len(words))\n",
    "    numero_palabras = 0\n",
    "    for word, values in words.items():\n",
    "        numero_palabras += values['freq']\n",
    "    total_palabras_doc.append(numero_palabras)\n",
    "\n",
    "#Imprimir Solo un documento\n",
    "print(\"El documento '{}' tiene {} palabras unicas despues del proceso de limpieza\".format(x_doc[0][-15:], y_doc[0]))\n",
    "print(\"El total de palabras contenido en el texto es de {}\".format(total_palabras_doc[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "palabras_raras = collections.OrderedDict()\n",
    "palabras_tres_caracteres = collections.OrderedDict()\n",
    "i = 0\n",
    "for documento, words in bag_of_words.items():\n",
    "    for word, values in words.items():\n",
    "        if len(word) > 10 and values['freq'] < 2:\n",
    "            palabras_raras[i] = word\n",
    "            i+=1\n",
    "        if bool(re.search(r'((\\w)\\2{2,})', word)):\n",
    "            palabras_tres_caracteres[word] = values['idf']\n",
    "palabras_raras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "palabras_tres_caracteres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cosas Varias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Buscar documentos con X palabra\n",
    "#palabras\n",
    "raras = []\n",
    "#bbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbb\n",
    "for documento, words in bag_of_words.items():\n",
    "    if 'integrationsreihenfolg' in words:\n",
    "        raras.append(documento)\n",
    "\n",
    "#buscar palabrasde mas de 15 caracteres       \n",
    "for palabra in palabras:\n",
    "    if len(palabra) > 15:\n",
    "        print(palabra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#/opt/datasets/mcda-pi1-20191/papers-txt/1501.02741.txt\n",
    "bag_of_words['/opt/datasets/mcda-pi1-20191/papers-txt/1501.02741.txt']['user']\n",
    "\n",
    "input_file = open('/opt/datasets/mcda-pi1-20191/papers-txt/1501.02741.txt',\"r\",encoding='utf-8')\n",
    "texto = input_file.read()\n",
    "#texto = re.sub('[^A-Za-z0-9]+',' ',texto) # Caracteres especiales\n",
    "tokens = texto.split()\n",
    "stemmer = PorterStemmer()\n",
    "stemmer2 = LancasterStemmer()\n",
    "tokens = [wordnet_lemmatizer.lemmatize(stemmer.stem(w.lower()), pos=\"v\") for w in tokens if (len(w)>1) and w.isalpha() and w not in sw] # Longitud mayor a 1\n",
    "counter=collections.Counter(tokens)\n",
    "dict(counter)['einhäuser']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Traductor\n",
    "def detect_language(text):\n",
    "\n",
    "    languages_ratios = {}\n",
    "    tokens = tokens = texto.split()\n",
    "    tokens = [w.lower() for w in tokens]\n",
    "    # Compute per language included in nltk number of unique stopwords appearing in analyzed text\n",
    "    for language in stopwords.fileids():\n",
    "        stopwords_set = set(stopwords.words(language))\n",
    "        words_set = set(tokens)\n",
    "        common_elements = words_set.intersection(stopwords_set)\n",
    "        languages_ratios[language] = len(common_elements) # language \"score\"\n",
    "    return max(languages_ratios, key=languages_ratios.get)\n",
    "\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "input_file = open(files_txt[0],\"r\",encoding='utf-8')\n",
    "texto = input_file.read()\n",
    "language = detect_language(texto)\n",
    "language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sw"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.6(conda)",
   "language": "python",
   "name": "python36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
